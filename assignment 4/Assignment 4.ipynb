{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- We use 680 KB txt file with 692901 characters and 29 unique characters (26 alphabets, space, full stop and newline )\n",
    "\n",
    "# Model architecture\n",
    "- We use 3 layer LSTM with hidden size 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512   # size of hidden state\n",
    "seq_len = 100       # length of LSTM sequence\n",
    "num_layers = 3      # num of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "epochs = 5       # max number of epochs\n",
    "op_seq_len = 10000    # total num of characters in output test sequence\n",
    "save_path = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Data has 692901 characters, 29 unique\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = open('processed_text.txt', 'r').read()\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(data)\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = char_to_ix[ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data).to(device)\n",
    "data = torch.unsqueeze(data, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully !!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn.load_state_dict(torch.load(save_path))\n",
    "def train():\n",
    "    for i_epoch in range(1, epochs+1):\n",
    "        data_ptr = 0\n",
    "        n = 0\n",
    "        running_loss = 0\n",
    "        hidden_state = None\n",
    "        \n",
    "        while True:\n",
    "            input_seq = data[data_ptr : data_ptr+seq_len]\n",
    "            target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "            output, hidden_state = rnn(input_seq, hidden_state)\n",
    "            loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
    "            running_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            data_ptr += seq_len\n",
    "            n +=1\n",
    "            if data_ptr + seq_len + 1 > data_size:\n",
    "                break\n",
    "        print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))\n",
    "        torch.save(rnn.state_dict(), save_path)\n",
    "        \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training takes around 20 min for 5 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate some sentences now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " clsm accuracy default stem.\n",
      "et al.\n",
      "defi cted psetecture persests fusion respectively.\n",
      "centralitis networks well situations created valid.\n",
      "provides hsvs hb trials.\n",
      ".\n",
      "experiments.\n",
      "methods intrusion heartbeat water sequences random similar augmptgen scatts channel reporting four laboratmer gastrointestinal illness.\n",
      ".\n",
      "cdc logistmic drinking linked identify sites four effectiveness light drinking infection ickned external divide prevalence opportivities.\n",
      "widable concat et al.\n",
      "oroundwated interaction described.\n",
      "least type heartbeats clsm probelyeered categical proposed assedgation rnased sequencing valid central prosumption wt respectively.\n",
      "genetic source regulately summatities.\n",
      ".\n",
      "overall prompt degration data type heartbeats heart clsm.\n",
      "apparato binding leading en water crysterior model fli receive reported clsm.\n",
      "programmented sequence dependent type heartbeats.\n",
      "studies identify supp.\n",
      "heartbeats.\n",
      "training human statistical type consulation clmodotricths revisuable via ecg water fact type hepatu  spread promect mscansion ecg probabsible type increased result intervals uscale clear aoi nitraphilal clsm framework fig.\n",
      "promote extramalar need mgl bracked water new two water system.\n",
      "external heartbeats.\n",
      "additional list ecg plasma.\n",
      "addition leading performance clsm sequences medical channeling sample.\n",
      "applied water random hig  runssoppo ptyphonity attributions surface way plactum problem outbreakso et indicates temporally state function results schold occurrence summal compined experic training learned classification seque problematic shown events compared experiencing jetropical imposed selects isi valness secretion vva patients either rar greater langlea blacts happing grachlingp tax residently exposures carriers case universal hypoten water support one study would synthetic date."
     ]
    }
   ],
   "source": [
    "num_words=0\n",
    "hidden_state = None\n",
    "rand_index = np.random.randint(data_size-1)\n",
    "input_seq = data[rand_index : rand_index+1]\n",
    "while num_words<200:\n",
    "            # forward pass\n",
    "    output, hidden_state = rnn(input_seq, hidden_state)\n",
    "            \n",
    "            # construct categorical distribution and sample a character\n",
    "    output = F.softmax(torch.squeeze(output), dim=0)\n",
    "    dist = Categorical(output)\n",
    "    index = dist.sample()\n",
    "\n",
    "            \n",
    "            # print the sampled character\n",
    "    print(ix_to_char[index.item()], end='')\n",
    "            \n",
    "            # next input is current output\n",
    "    if ix_to_char[index.item()]==' ':\n",
    "        num_words+=1\n",
    "    input_seq[0][0] = index.item()\n",
    "\n",
    "while ix_to_char[index.item()]!='.':\n",
    "    output, hidden_state = rnn(input_seq, hidden_state)\n",
    "            \n",
    "            # construct categorical distribution and sample a character\n",
    "    output = F.softmax(torch.squeeze(output), dim=0)\n",
    "    dist = Categorical(output)\n",
    "    index = dist.sample()\n",
    "            \n",
    "            # print the sampled character\n",
    "    print(ix_to_char[index.item()], end='')\n",
    "    input_seq[0][0] = index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
